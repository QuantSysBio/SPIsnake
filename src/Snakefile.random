"""
Run with:

snakemake -s Snakefile.random -j 1 make_all_files
"""
import random
import string

seed = 1
random.seed(seed)

# first rule: generate a spreadsheet with some random strings in it.
# (in a real workflow, this would be something like, "run a program
# that creates a spreadsheet with a dynamic number of entries in it")
rule make_spreadsheet:
    output: "names.csv"
    run:
        num = random.uniform(4, 15)
        letters = string.ascii_lowercase
        with open(str(output), 'wt') as fp:
            for n in range(0, num):
                rs = ''.join(random.choice(letters) for i in range(10))
                print(rs, file=fp)

# second rule, a checkpoint for rules that depend on contents of "count.csv"
checkpoint check_csv:
    input: "names.csv"
    output:
        touch(".make_spreadsheet.touch")

# checkpoint code to read count and specify all the outputs
class Checkpoint_MakePattern:
    def __init__(self, pattern):
        self.pattern = pattern

    def get_names(self):
        with open('names.csv', 'rt') as fp:
            names = [ x.rstrip() for x in fp ]
        return names

    def __call__(self, w):
        global checkpoints

        # wait for the results of 'check_csv'; this will trigger an
        # exception until that rule has been run.
        checkpoints.check_csv.get(**w)

        # the magic, such as it is, happens here: we create the
        # information used to expand the pattern, using arbitrary
        # Python code.
        names = self.get_names()

        pattern = expand(self.pattern, name=names, **w)
        return pattern

rule make_all_files:
    input:
        Checkpoint_MakePattern("output-{name}.txt")

# final rule: make a file of the given format.
rule make_file:
    output:
        "output-{something}.txt"  # doesn't matter what "something" is :)
    shell:
        "echo hello, world > {output}"


def get_cutadapt_input(wildcards):
    MT = Master_table_expanded.loc[wildcards.chunk]

        return expand(
            "pipe/cutadapt/{S}/{U}.{{read}}.fastq{E}".format(
                S=unit.sample_name, U=unit.unit_name, E=ending
            ),
            read=["fq1", "fq2"],
        )

join(dir_DB_exhaustive, "Peptides/cutadapt/{S}/{U}.{{read}}.fastq{E}".format(
                S=unit.sample_name, U=unit.unit_name, E=ending
            ))








rule Split_proteome_chunks:
    """
    Splits input proteome into chunks of approx similar volume for DB construction
    """
    input:
        proteome = features["reference"]["reference_proteome_fasta"],
        Nmers = features["DB"]["k_mers"],
        MiSl = features["DB"]["max_intervening_length"],
        min_protein_length = features["DB"]["min_protein_length"],
        maxE = features["DB"]["maxE"],
        functions = "src/snakefiles/functions.R"
    output:
        orderedProteomeEntries = "results/DB_exhaustive/orderedProteomeEntries.RData",
        P = "results/P.RData"
    benchmark: 
        "results/benchmarks/Split_proteome_chunks.json"
    log: 
        "results/logs/Split_proteome_chunks.txt"
    conda: 
        "R_env.yaml"
    params:
        n=config["max_cores"],
        Output_dir="results/"
    script:
        "02_1_Split_proteome_chunks.R"


Master_table_expanded = (
pd.read_csv(os.path.join(dir_DB_exhaustive, "Master_table_expanded.csv"), 
    sep=",", 
    dtype={
    "Proteome": str, 
    "Splice_type": str, 
    "N_mers": int, 
    "Min_Interv_length": int, 
    "chunk": str})
.set_index(["Proteome", "Splice_type", "N_mers", "Min_Interv_length", "chunk"], drop=False)
.sort_index()
)

def get_Master_table():
    Master_table_expanded = (
    pd.read_csv(os.path.join(dir_DB_exhaustive, "Master_table_expanded.csv"), 
        sep=",", 
        dtype={
        "Proteome": str, 
        "Splice_type": str, 
        "N_mers": int, 
        "Min_Interv_length": int, 
        "chunk": str})
    .set_index(["Proteome", "Splice_type", "N_mers", "Min_Interv_length", "chunk"], drop=False)
    .sort_index()
    )
    return Master_table_expanded




rule make_file:
    input:
        chunk = lambda wildcards: Master_table_expanded.loc[wildcards.chunk, 'chunk']
    output:
        join(dir_DB_exhaustive, "{wildcards.chunk}.txt"),
        touch(join(dir_DB_exhaustive, ".make_file.done"))
    params:
        Proteome = lambda wildcards: Master_table_expanded.loc[wildcards.chunk, 'Proteome'],
        Splice_type = lambda wildcards: Master_table_expanded.loc[wildcards.chunk, 'Splice_type'],
        N_mers = lambda wildcards: Master_table_expanded.loc[wildcards.chunk, 'N_mers'],
        Min_interv_length = lambda wildcards: Master_table_expanded.loc[wildcards.chunk, 'Min_interv_length']
    shell:
        "echo {wildcards.chunk} > {output}.txt"